{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKQcWPsvonK1"
      },
      "source": [
        "# <font color='blue'>**LABORATORIO 2. Clasificación de imágenes usando CNN's**\n",
        "\n",
        "<font color='red'>Integrantes (Máximo 4 personas):\n",
        "\n",
        "<font color='red'>Nombres y Apellidos\n",
        "\n",
        "<font color='red'>Codigo UAO\n",
        "\n",
        "Estudiar las Redes Neuronales Convolucionales (CNN) es beneficioso por varias razones:\n",
        "\n",
        "* **Excelente Desempeño en Imágenes**: Las CNN han demostrado un rendimiento excepcional en tareas de visión computacional debido a su capacidad para capturar patrones locales y globales en imágenes.\n",
        "\n",
        "* **Eficiencia en Términos de Parámetros**: Las CNN están diseñadas para aprovechar la estructura de las imágenes, utilizando capas convolucionales que comparten parámetros. Esto las hace mucho más eficientes en términos de la cantidad de parámetros requeridos en comparación con una red neuronal *FULLY CONNECTED*.\n",
        "\n",
        "* **Invariancia a Traslaciones**: Las capas convolucionales permiten a las CNN aprender características invariantes a pequeñas traslaciones. Esto significa que la red puede reconocer patrones incluso si están ubicados en diferentes partes de una imagen.\n",
        "\n",
        "* **Capacidad de Aprendizaje Jerárquico**: Las CNN tienen múltiples capas convolucionales y de agrupación que les permiten aprender características a diferentes niveles de abstracción. Las primeras capas pueden aprender bordes y texturas simples, mientras que las capas más profundas pueden aprender características más complejas y abstractas.\n",
        "\n",
        "* **Transferencia de Conocimiento**: Las CNN entrenadas en grandes conjuntos de datos, como ImageNet, pueden ser utilizadas como puntos de partida para tareas de clasificación específicas. Esto se conoce como transferencia de conocimiento y puede ahorrar mucho tiempo y recursos en el entrenamiento de modelos.\n",
        "\n",
        "* **Interpretabilidad Relativa**: Aunque las CNN no son tan interpretables como algunos otros modelos más simples, como árboles de decisión, se han desarrollado técnicas y herramientas para ayudar a comprender qué características están siendo detectadas por diferentes capas de la red.\n",
        "\n",
        "Siga las instrucciones (<font color='red'>resaltadas en rojo </font>), las cuales además de guiarlo, generarán en suma el puntaje final obtenido para este laboratorio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4a2HzpMM27y"
      },
      "source": [
        "#Para dar un orden al trabajo realizado, coloque aqui todas las librerias que van a ser usadas.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXofP_DqrIzi"
      },
      "source": [
        "# Vamos a usar el Human Action Recognition (HAR) Dataset de Kaggle\n",
        "\n",
        "!kaggle datasets download -d meetnagadia/human-action-recognition-har-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8pow6p77Eaj"
      },
      "source": [
        "<font color='red'>Punto 1 (1.0). Preprocesamiento</font>.\n",
        "\n",
        "El pre-procesamiento de imagenes es una tarea fundamental cuando se desea que la IA trabaje correctamente.\n",
        "\n",
        "(a) (0.25) Realice un código que le permita mostrar 5 imagenes por cada categoría seleccionadas del dataset.\n",
        "\n",
        "(b) (0.25) Aunque el dataset ya se encuentra dividido en TRAIN y TEST, a partir del análisis visual realizado en el punto (a), usted deberá tomar la decisión acerca de trabajar el entrenamiento con todo el conjunto de imagenes dadas en TRAIN o con un subconjunto del mismo. Por otro lado usted deberá separar el 10% de las imagenes de TEST y dejarlas como una nueva carpeta de VALIDATION.\n",
        "\n",
        "(c) (0.5) Realice un código que le permita pre-procesar el dataset. <font color='red'>Si usted decide no hacer ningun tipo de pre-procesamiento, usted deberá explicar claramente el porque usted consideró no necesario realizar este paso.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26-95aO59heO"
      },
      "source": [
        "#Pegue y ejecute  aquí el código para el punto 1a.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-tAM7v7kb0"
      },
      "source": [
        "#Pegue y ejecute aquí el código para el punto 1b.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Si va a realizar codigo para pre.procesamiento, pegue el codigo aqui.\n",
        "#De lo contrario haga uso de una pesataña de texto para explicar las razones de no hacerlo."
      ],
      "metadata": {
        "id": "M8YYYzgBbR-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Punto 2 (1.0). Cree y entrene un modelo para clasificación usando como base la arquitectura VGG16.\n",
        "\n",
        "(0.5) Utilice los conocimientos adquiridos para entrenar y validar su modelo. Usted deberá hacer uso de: Gráficas de Accuracy / Loss, MaAtriz de confusión y reporte de clasificación (F1-Score, Recall, Precision)\n",
        "\n",
        "(0.5) Analice los resultados obtenidos con las diferentes herramientas, dando una explicación que incluya las difernetes métricas y que sean consecuentes con los reultados obtenidos."
      ],
      "metadata": {
        "id": "gKt1YXfVlg8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pegue y ejecute aquí su código para el punto 2.1"
      ],
      "metadata": {
        "id": "9BmnW8OC2nSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edite aquí el análisis para el punto 2.2"
      ],
      "metadata": {
        "id": "PNjxSmZDzBaF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G-Vpw2q8I9I"
      },
      "source": [
        "<font color='red'>Punto 3 (2.0). Usando métodos de regularización para mejorar el \"***performance***\" de su  modelo base.\n",
        "\n",
        "(1.0) Usted podrá combinar diferentes métricas de regularización con el fin de obtener 3 modelos (**modelos regularizados**) que superen los resultados del modelo base.\n",
        "<font color='red'> [Nota: Los 3 modelos deben mostrar graficos de Acurracy que corroboren la eliminación del overfitting. Tenga en cuenta además que los 3 modelos deberán mantener un Val-accuracy superior al 70%]\n",
        "\n",
        "(0.25) Haciendo uso de transfer learning entrene un cuarto modelo. <font color='red'> [Nota: No olvide mostrar el grafico de Acurracy que corrobore la eliminación del overfitting y mantener un Val-accuracy superior al 70%]\n",
        "\n",
        "(0.75) Realice un análisis comparativo de los resultados obtenidos con el mejor de los 3 modelos regularizados y el modelo 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqQ4U_A1A7Xs"
      },
      "source": [
        "#Pegue y ejecute aquí su código para el punto 3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy7WmWB2H4vQ"
      },
      "source": [
        "#Pegue y ejecute aquí su código para el punto 3.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (0.5) Pegue y ejecute aquí su código para el punto 3.3"
      ],
      "metadata": {
        "id": "CkCMiKFizn7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(0.25) Edite aqui el analisis del punto 3.3"
      ],
      "metadata": {
        "id": "rozMYXFCzaCm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTE9zYBvJy9P"
      },
      "source": [
        "<font color='red'>Punto 4 (1.0). Comprendiendo mejor su Clasificador</font>.\n",
        "\n",
        "A partir de la lectura juiciosa de este enlace: [VISUALIZING FEATURE MAPS](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/).\n",
        "\n",
        "Realice una comparación visual de las diferentes capas convolucionales (entendiendo que estas, son las que producen los diferentes feature maps).<font color='red'>[Nota: Para esta parte, utilice el conjunto de datos de TEST]</font>.\n",
        "\n",
        "Acompañe esta comparación visual con un párrafo donde muestre su análisis al respecto.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pegue y ejecute aquí su código."
      ],
      "metadata": {
        "id": "03iaDPjZAHoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edite aquí su analisis."
      ],
      "metadata": {
        "id": "mCR6bi0Q1rxd"
      }
    }
  ]
}